# Deep Learning - HW1

1. В рамках данной работы было необходимо реализовать `CNN` с использованием `Numpy` и протестировать решение на датасете `MNIST`. Кроме того, необходимо реализовать `ResNet50` и протестировать модель на датасете `Car Dataset` с использованием `Adam` и `Adamax`.
2. Модуль nn содержит классы и функции для нейронной сети написанной на numpy. Были реализованы `Linear, Conv2d, MaxPool2d, AvgPool2d, Flatten, Softmax` слои, которые в качестве данных хранят необходимые параметры слоя (например матрица весов W и биасы b в Linear слое). Базово все слои реализуют два метода - `forward` и `backward`. Метод `forward` принимает на вход `np.array` и применяет к нему некую трансформацию. Метод же `backward` принимает на вход производную по выход данного слоя и обновляет все необходимые веса + считает производную по входу для продолжения обратного распространения ошибки.
3. В [deep_learning_hw_np_model.ipynb](./deep_learning_hw_np_model.ipynb) содержится построение модели и код обучения кастомной нейронной сети на датасете `MNIST`. Разработанная архитектура хорошо справилась с классификацией цифр. Результаты классификации, confusion matrix и график падения ошибки можно увидеть в самом ноутбуке.
4. В [deep-learning-hw1-pytorch.ipynb](./deep-learning-hw1-pytorch.ipynb) содержится построение и обучение `ResNet50` с различными оптимизаторами (`Adam, Adamax`). Семейство `ResNet` в свое время показало отличные результаты на датасете `ImageNet`, благодаря использованию **residual блоков со skip-connection** (метод, позволяющий справится с проблемой затухающих градиентов за счет пробрасывания входной информации дальше по сети с помощью суммы или конкатенации). К сожалению, модель очень сильно переобучалась и увеличение `weight_decay` не сильно спасло ситуацию. Возможно, большее число итераций и больший `weight_decay` мог бы улучшить ситуацию. + хотелось бы добавить дополнительную регуляризацию с использованием `Dropout` слоев, но в оригинальном `Resnet50` их не было. Кроме того, видно, что `Adamax` улучшает сходимость в отличии от `Adam`, однако метрики при этом ниже - возможно Adamax при других экспериментах потребовал бы еще больше регуляризации модели.
5. Источники:
  * [ResNet50 architecture](https://iq.opengenus.org/resnet50-architecture/)
  * [ResNet50 arxiv](https://arxiv.org/abs/1512.03385)
  * [Adam, Adamax](https://arxiv.org/abs/1412.6980)
  * [Adamax Pytorch](https://www.google.com/search?client=safari&rls=en&q=Adamax+pytorch&ie=UTF-8&oe=UTF-8)